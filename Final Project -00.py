# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EYQu9XvjR8Zm7pw1lMJbN9FO-1Z0ZEIU
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from sklearn.utils import shuffle
from wordcloud import WordCloud
# %matplotlib inline
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense,Bidirectional,Activation,Lambda,Input,Embedding,LSTM

df=pd.read_csv('/content/hindi_english_parallel.csv')
print(df.head())

print(len(df))

# Checking for Null values
df.isnull().sum()

# Dropping null values and resetting the index
df.dropna(inplace=True)
df=df.reset_index(drop=True)

df.isnull().sum()

len(df)

df['english']

# There are some rows in english columns that contains hindi
df.iloc[155570]

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
stop_words = stopwords.words('english')

import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Make sure to use the correct column name for English
corpus_list = []
for line in df['english']:   # change 'english' to your actual column name
    cleaned_line = re.sub(r'[^A-Za-z\s]', '', str(line))  # keep only English letters
    cleaned_line = ' '.join(cleaned_line.split())          # remove extra spaces
    corpus_list.append(cleaned_line)

corpus = ' '.join(corpus_list)

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(corpus)

plt.figure(figsize=(13,6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Removing the rows that contains hindi sentences as it will just create noise in the dataset
def contains_Hindi(text):
    if isinstance(text,str):
        return bool(re.search(r'[\u0900-\u097F]',text))
    return False
df_filtered=df[~df['english'].apply(contains_Hindi)].reset_index(drop=True)

Embedding_dim=100
Latent_dim=400
max_seq_len=100
max_vocab_size=20000

# As the data size is large we will work with 20000 sentences
input_text=[]
target_text=[]
target_text_input=[]
translation=[]
i=19999
for j,t in enumerate(df['english']):
    input_text.append(t)
    if j==i:
        j=0
        break
for j,t in enumerate(df['hindi']):
    translation.append(t)
    target_text.append(t+' <eos>')
    target_text_input.append('<sos> '+t)
    if j==i:
        break

#Input
tokenizer_input=Tokenizer(num_words=max_vocab_size)
tokenizer_input.fit_on_texts(input_text)
tokenized_input=tokenizer_input.texts_to_sequences(input_text)

word2idx=tokenizer_input.word_index
print('word2idx length:',len(word2idx))

#Output
tokenizer_output=Tokenizer(num_words=max_vocab_size,filters='')
tokenizer_output.fit_on_texts(target_text_input+target_text)
tokenized_target=tokenizer_output.texts_to_sequences(target_text)              #target output
tokenized_target_input=tokenizer_output.texts_to_sequences(target_text_input)  #teacher forcing

word2idx_output=tokenizer_output.word_index
print('length of word2idx output:',len(word2idx_output))
num_words_output=len(word2idx_output)+1

import zipfile

# Extract the zip
with zipfile.ZipFile("/content/glove.6B.100d.txt.zip", 'r') as zip_ref:
    zip_ref.extractall("/content/")

# Now read the extracted file
word_vec = {}
with open("/content/glove.6B.100d.txt", encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector = np.asarray(values[1:], dtype='float32')
        word_vec[word] = vector

print('length of word vector =', len(word_vec))

#Input
encoder_input=pad_sequences(tokenized_input,maxlen=max_seq_len)
print('Encoder input shape=',encoder_input.shape)

#target output as input for teacher forcing
decoder_input=pad_sequences(tokenized_target_input,maxlen=max_seq_len,padding='post')
print('decoder input shape:',decoder_input.shape)

#target output
decoder_output=pad_sequences(tokenized_target,maxlen=max_seq_len,padding='post')
print('decoder output shape:',decoder_output.shape)

#creating count matrix
num_words=min(max_vocab_size,len(word2idx)+1)
word_embedding=np.zeros((num_words,Embedding_dim))
for word,i in word2idx.items():
    if i<max_vocab_size:
        vec=word_vec.get(word)
        if vec is not None:
            word_embedding[i]=vec
print('shape of word_embedding:',word_embedding.shape)

embedding_layer_input=Embedding(num_words,
                          Embedding_dim,
                          weights=[word_embedding],
                          trainable=False)

#Encoder
encoder_input_placeholder=Input(shape=(max_seq_len,))
x=embedding_layer_input(encoder_input_placeholder)
encoder_lstm=LSTM(Latent_dim,return_state=True)
encoder_output,h,c=encoder_lstm(x)
encoder_states=[h,c]

#Decoder
decoder_input_placeholder=Input(shape=(max_seq_len,))
decoder_embedding=Embedding(num_words_output,Embedding_dim)
x=decoder_embedding(decoder_input_placeholder)
decoder_lstm=LSTM(Latent_dim,return_state=True,return_sequences=True)
decoder_outputs,_,_=decoder_lstm(x,initial_state=encoder_states)
decoder_dense=Dense(num_words_output,activation='softmax')
decoder_outputs=decoder_dense(decoder_outputs)

model=Model([encoder_input_placeholder,decoder_input_placeholder],      # Teacher Forcing
            decoder_outputs)

model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

hist=model.fit([encoder_input,decoder_input],
               decoder_output,
               epochs=100,
               validation_split=0.2,
               batch_size=64)

plt.plot(hist.history['val_accuracy'],label='val_acc')
plt.plot(hist.history['accuracy'],label='accuracy')
plt.legend()

plt.plot(hist.history['val_loss'],label='val_loss')
plt.plot(hist.history['loss'],label='loss')
plt.legend()

encoder_model=Model(encoder_input_placeholder,encoder_states)

# Decoder for prediction (one input at a time)
decoder_inital_h=Input(shape=(Latent_dim,))
decoder_inital_c=Input(shape=(Latent_dim,))

decoder_initial_states=[decoder_inital_h,decoder_inital_c]

decoder_input_single=Input(shape=(1,))
decoder_embedding_single=decoder_embedding(decoder_input_single)
decoder_outputs_x,h,c=decoder_lstm(decoder_embedding_single,initial_state=decoder_initial_states)

decoder_states=[h,c]

decoder_output=decoder_dense(decoder_outputs_x)

decoder_model=Model([decoder_input_single]  +  decoder_initial_states,
                    [decoder_output]  +  decoder_states
                   )

idx2word_output={v:k for k,v in word2idx_output.items()}

def decode_sequence(sequence):
    states_value=encoder_model.predict(sequence,verbose=0)     # context vector:-will return [h,c]

    target_seq=np.zeros((1,1))
    target_seq[0,0]=word2idx_output['<sos>']    #First input to decoder

    eos=word2idx_output['<eos>']

    output_sequence=[]

    for _ in range(max_seq_len):
        output_token,h,c=decoder_model.predict([target_seq]+states_value,verbose=0)

        idx=np.argmax(output_token[0,0,:])

        if idx==eos:                       # if the predicted token is <eos>
            break

        if idx>0:
            output_sequence.append(idx2word_output[idx])

        target_seq[0,0]=idx

        states_value=[h,c]

    return ' '.join(output_sequence)

i=0
while i<10:
    j=np.random.choice(len(encoder_input))
    input_seq=encoder_input[j:j+1]
    seq=decode_sequence(input_seq)
    print('------------------')
    print('input text:',input_text[j])
    print('real output',translation[j])
    print('predicted output:',seq)
    i+=1

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import re

# Smoothing method better for short sentences
smooth = SmoothingFunction().method5

# Function to clean text: lowercase, strip, remove punctuation
def clean_text(text):
    text = text.lower().strip()
    text = re.sub(r"[^\w\s]", "", text)  # remove punctuation
    return text

bleu_scores = []
for ref, pred in zip([translation[i] for i in sample_indices], predicted_outputs):
    ref_clean = clean_text(ref)
    pred_clean = clean_text(pred)

    # Weighted n-grams: emphasize 1-gram and 2-gram for short sentences
    bleu = sentence_bleu(
        [ref_clean.split()],
        pred_clean.split(),
        weights=(0.5, 0.5, 0, 0),
        smoothing_function=smooth
    )
    bleu_scores.append(bleu)

avg_bleu = sum(bleu_scores) / len(bleu_scores)
print(f"Average BLEU: {avg_bleu:.4f}")